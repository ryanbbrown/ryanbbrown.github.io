# Company Overview
Dell Technologies is a multinational technology company that is well known for its computers and monitors but also offers, and technology solutions including servers, storage, and various services. 

Dell Services is the largest branch of Dell Services and encompasses consulting, deployment, support, managed, and education services. I was on the Services Data Solutions (SDS) team, which works to enable data science users throughout the entirety of Dell Services.


# Project Overview
My internship focused on the vast usage logs generated by the Azure hybrid cloud ecosystem that SDS manages; there were petabytes of unexplored usage logs and thus an emerging opportunity to discover key findings and harness the usage logs to continuously improve the ecosystem. 

I was given guidelines for how to find the many kinds of usage logs and a general goal of improving the ecosystem, but beyond that, the project was very open and the specifics were left to my discretion. 

I had to break down the broad, complex problem to determine what data was useful, how to analyze it, and what kinds of solutions I could deliver to improve the ecosystem. Ultimately, I created a platform for value generation within the ecosystem that aimed to directly improve the value already provided to users as well as increase the frequency and efficiency with which the core team (Services Data Solutions) was able to improve the ecosystem.


# Platform Components
My platform has three main components: an Azure Anomaly Detection Model, a SQL Query Network Graph, and a Databricks Usage Dashboard.

## Azure Cost Anomaly Detection Model
The Anomaly Detection Model's purpose is to perform time-series forecasting and subsequent anomaly detection for the cost of each of the combinations of team and product within Dell Services. Some examples of notable products include virtual machines, data lake storage, Databricks, and Synapse Analytics. Each time an anomaly is detected, an alert is automatically sent to a Microsoft Teams channel; SDS can then act to understand the anomaly and reduce costs in the future.

The proof-of-concept for the anomaly detection model was done using Facebook's Prophet model in Python on historical cost data. It showed that the anomaly detection model could drive cost savings of up to 10% monthly for a team's product usage. After the successful POC, the production model was created in Azure Metrics Advisor for easy integration with the cost data. 

## SQL Query Network Graph
The SQL Query Network Graph clearly visualizes the relationship between tables, users, and queries in Databricks SQL using a network visualization in PowerBI. It allows users that are new to a table to see who else is querying the table and what they're joining it with, reducing redundancy and improving the efficiency of data science users by leveraging the existing knowledge base. It also gives SDS a unique view of what data science users are doing and how they're related.

The visualization is powered by a pipeline created using Databricks and Azure Data Factory that curates the existing query history dataset (which I collected from the API and curated + stored using another pipeline).

In addition to the PowerBI report, I set up automated weekly reports to be sent out to users in Microsoft Teams with information about tables they queried, other users querying the same tables, and tables that were joined to the tables they queried. These reports proactively ensure that the benefits from the SQL Query Network reach as many users as possible; users that think they're optimally querying a table won't seek out the query network, but may still benefit from knowledge share or collaboration.

## Databricks Usage Dashboard
The Databricks Usage Dashboard has two primary purposes: the first is to generally improve visibility about what Dell Services data science users are doing in Databricks, both in the SQL workspace and in the Data Science & Engineering (DS+E) workspace. The second is to power analysis and insight generation by the core team; trends, patterns, and behavior in the usage data are much more quickly and easily identifiable, and can be acted on using the curated datasets.

The dashboard was created in PowerBI and is powered by four pipelines that I engineered using Databricks and Azure Data Factory. These pipelines collect the usage log data, either from raw storage archives or from APIs, curate it to a form that can be analyzed + visualized, and send it to a production storage location to be accessed by the dashboard and by SDS team members for further analysis.

As a proof-of-concept of the dashboard's ability to shorten the insight generation process from weeks to days, I used it to identify why user logins were constant over time. The analysis ultimately led to the categorization of three main user types, with the category of interest being churned users. This analysis was completed in a single day, and I set up the infrastructure for the team to automatically categorize users + be alerted when a user churns in the future.
